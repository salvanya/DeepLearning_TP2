# Academic project: Audio recognition & Fake news generation

This repository contains two academic exercises that demonstrate solutions to Deep Learning problems using various techniques and models implemented in Python with TensorFlow and Keras. The first exercise focuses on **audio recognition**, utilizing convolutional neural networks (CNNs) applied to spectrograms of spoken digits from audio clips. The second exercise explores **text generation** using recurrent neural networks (RNNs) with LSTM cells, comparing tokenization at the word and character levels to create new fake news articles based on the AG News Dataset. Each project highlights the implementation of neural network models in distinct scenarios, showcasing their versatility and effectiveness.

## Exercise 1 - Audio MNIST:

In this exercise, the task is to classify spoken digits from audio clips. The dataset used is the [Spoken Digit Dataset](https://www.tensorflow.org/datasets/catalog/spoken_digit), which contains a total of 2500 audio clips representing the digits 0 through 9, spoken by 5 different speakers, with each speaker providing 50 clips per digit. The objective is to develop two neural network models that can accurately identify the spoken digit from these clips. The first model is a convolutional neural network (CNN) that uses the spectrograms of the audio clips, while the second model is a recurrent neural network (RNN), also based on spectrograms. By comparing these models, the exercise aims to explore the effectiveness of different neural network architectures in the domain of audio-based digit classification. The results are expected to provide insights into which model architecture is better suited for this task, with a focus on performance metrics and relevant visualizations.

The file containing the solution of this exercise is [`audio_calssification_using_RNN.ipynb`](https://github.com/salvanya/DeepLearning_TP2/blob/main/audio_calssification_using_RNN.ipynb)

## Exercise 2 - Fake News:

This exercise focuses on generating fake news articles using text generation models. The dataset used is the [AG News Subset](https://www.tensorflow.org/datasets/catalog/ag_news_subset), which contains 120,000 news article summaries from four different categories. Although the categories are not relevant for this task, the dataset serves as a comprehensive text corpus for training the models. The goal is to build two types of text generation models: a character-level model and a word-level model. The character-level model generates text one character at a time, while the word-level model generates text one word at a time. The objective is to qualitatively compare the articles generated by each model, highlight the strengths and weaknesses of each approach in generating coherent and plausible text based on the provided dataset.

The file containing the solution of this exercise is [`text_generation.ipynb`](https://github.com/salvanya/DeepLearning_TP2/blob/main/text_generation.ipynb)


## Set up

1. Run in terminal:
    
    `pip3 install -r requirements.txt`

2. To explore the solutions:
    
    Simply run cells in order in both jupyter notebook files.
