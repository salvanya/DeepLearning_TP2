{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcOUxZaY60yn"
      },
      "source": [
        "# Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hes9xm5k8IL2",
        "outputId": "0e896e72-d136-4ee8-cba9-f188edb47b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# Instalar librerías necesarias\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vkRjNZgrKtFD"
      },
      "outputs": [],
      "source": [
        "# Importar librerías requeridas\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfCfUvmg6Ndl"
      },
      "source": [
        "# Sección 1: Preparación de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBSzRioQ6YCC"
      },
      "source": [
        "## 1.1 Cargar el Dataset\n",
        "\n",
        "El conjunto de datos de AG News es una colección de artículos de noticias, un subconjunto, del corpus de artículos de noticias de AG. Contiene 120.000 muestras de entrenamiento y 7.600 muestras de prueba. Cada muestra se clasifica en una de cuatro clases: Mundo, Deportes, Negocios y Ciencia/Tecnología. Para nuestra tarea de generación de texto, nos centraremos en el contenido del texto, sin tener en cuenta las categorías.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "37f81c66c40745bb9ab631e26497ce7b",
            "61b3f5b74437464493179b0a0ad4081c",
            "2e839e89cf644fad80c31bf57b0aae92",
            "678087494c8a4951893070cc3fe6c7b6",
            "04231bc8cfdf48a28f5adb56eada4476",
            "408c303a9669413baeac7d68bdf3c34d",
            "727134da2d334d60bede4a21c73532b2",
            "ca60b4543fb1411382d838e4a59f27e2",
            "472892fc0c634dfc8a419f0a4144b650",
            "7dfb7b6b00ed4655ae520da8da9348a5",
            "7e835ce69df1441698a68d1b0ac5752a",
            "069cdb3e96b446d59563ac978d288a7e",
            "8a2e8854a2fd4c02a03350164d143c40",
            "2a16d3de520e426f85cd6c32cbcd5928",
            "5f4954a9810f45c1848622e2a84afaab",
            "7927571e627240be85488981fb0f51e4",
            "b06117eea5f5488a9697f71b28404a62",
            "706cb46c74e7410fa98079212f6b4b01",
            "ff87dcb39f9749479ee6707021a15c41",
            "dd4a661a328c4d88b330cd429c473bdf",
            "2e4e8e2ca8e947c28ee7e669a64afa7c",
            "f643897de20b48fbad55fa3071f8a5ac",
            "961f29bcc8364c2887eca79ebb196882",
            "31493b0f488844b487a4fcd0ae7a75b0",
            "17f3cc1a23004343be7a7a0576574a4c",
            "0f8f14e9d2bd4d26818fb7034ef72c22",
            "8dfddee8ceb74dd09827b3a8a5141a81",
            "e68053b71b5241059a3e8c7bcb3a12fb",
            "2bceed6e959f40c880ded258ecfba5f2",
            "bd0288b0ccec49e387face63d9500fe4",
            "93af605ec66348aa950ca8339088ef1f",
            "02b55025498547ac87f6824e57de530e",
            "61a31bb7e49b40abacced828c1f42928",
            "cbf34038d780440ba5d41862c16e1c03",
            "ab2976587eed41cd9fe0cf22d649029e",
            "092ad9e2ce1746b8a23915d447266787",
            "86f22dbb5583476c99a129b05d1488c4",
            "7a5d7a153d4841c1ae0590cd8f5b5cf2",
            "e192ceaa760149b080b57209f2793860",
            "71d0ef96ab1f46f3845e760915022741",
            "a090f95d1aea43cf8d76e022a034b71f",
            "e25597fbb2da4c6e8a4dea142aa5441b",
            "71643f493a8348918bc8322707009b2e",
            "424f87b892a849679d46aaa72e8c4f41",
            "6e0e2e9b8cd7401fbcc87ec81e3dd96a",
            "4ed3c4f20ff244e39b3aa22a830e1c9e",
            "8984c1830a344060bc745986b620049a",
            "ab9efe27ccde44608feff505d30bd63e",
            "f644eafbbba545a6a06fb4bab7e8c5b7",
            "ce85de7855cd41eabfe242c453e32e48",
            "92ced3143a784edab482804388ef4111",
            "088a4bd7413d4a48b18f81536d7600b2",
            "51dd20394cd642cf81f6d075bc9f2717",
            "37c0eadd792d4a1cab68f709f4a73fe9",
            "009d9cccf1254057a65ad353aef4943d"
          ]
        },
        "id": "YmAT21jd6MQq",
        "outputId": "2c3dc0a6-f600-431b-c633-c2a4d40959ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37f81c66c40745bb9ab631e26497ce7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "069cdb3e96b446d59563ac978d288a7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "961f29bcc8364c2887eca79ebb196882",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbf34038d780440ba5d41862c16e1c03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e0e2e9b8cd7401fbcc87ec81e3dd96a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Cargar el dataset AG News\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "# Convertir a un dataframe de Pandas\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_test = pd.DataFrame(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Pls6jy6l4q",
        "outputId": "078aae47-a72c-4f49-b081-b249cf7b952e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Forma del conjunto de entrenamiento: (120000, 2)\n",
            "Forma del conjunto de prueba: (7600, 2)\n",
            "\n",
            "Ejemplo de algunas muestras:\n",
            "                                                text  label\n",
            "0  Wall St. Bears Claw Back Into the Black (Reute...      2\n",
            "1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n",
            "2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n",
            "3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n",
            "4  Oil prices soar to all-time record, posing new...      2\n",
            "\n",
            "Etiquetas únicas\n",
            "[2 3 1 0]\n"
          ]
        }
      ],
      "source": [
        "# Mostrar información básica del dataset\n",
        "print(f\"\\nForma del conjunto de entrenamiento: {df_train.shape}\")\n",
        "print(f\"Forma del conjunto de prueba: {df_test.shape}\")\n",
        "print(\"\\nEjemplo de algunas muestras:\")\n",
        "print(df_train.head())\n",
        "\n",
        "# Mostrar etiquetas únicas\n",
        "print(\"\\nEtiquetas únicas\")\n",
        "print(df_train['label'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bkUk8327oQU"
      },
      "source": [
        "## 1.2 Preprocesamiento de los datos\n",
        "El preprocesamiento es un paso crucial en las tareas de procesamiento del lenguaje natural.\n",
        "\n",
        "Para nuestro modelo de generación de texto, necesitamos limpiar los datos de texto y convertirlos a un formato adecuado para el entrenamiento.\n",
        "\n",
        "Esto implica eliminar los caracteres innecesarios, convertirlos a minúsculas y tokenizar el texto.\n",
        "\n",
        "Haremos el preprocesamiento en simultáneo para el modelado a nivel de caracteres y para el modelado a nivel de palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpw5uKLkSJ2r"
      },
      "source": [
        "Limpiar el texto de entrenamiento para un modelo de generación de texto puede mejorar la calidad y eficiencia del modelo, pero no siempre es necesario eliminar todos los signos de puntuación. La decisión de limpiar o no el texto depende de varios factores, incluyendo el propósito del modelo y el tipo de texto que se está procesando.\n",
        "\n",
        "**Ventajas de Mantener los Signos de Puntuación**\n",
        "\n",
        "1. Los signos de puntuación como comas, puntos y signos de exclamación proporcionan un contexto importante y afectan al significado del texto. Eliminar estos signos puede resultar en la pérdida de información significativa.\n",
        "\n",
        "2. En muchos casos, los signos de puntuación ayudan a estructurar el texto de manera coherente y fluida, lo cual es crucial para la generación de texto natural y legible.\n",
        "\n",
        "3. Mantener los signos de puntuación puede ayudar al modelo a aprender a generar texto que suene más natural y esté mejor estructurado, lo que es particularmente útil en tareas de generación de texto como la escritura creativa o la generación de respuestas en chatbots.\n",
        "\n",
        "**Ventajas de Limpiar los Signos de Puntuación**\n",
        "\n",
        "1. Al eliminar signos de puntuación, el espacio de entrada se reduce, lo que puede simplificar el modelo y disminuir la carga computacional, especialmente en textos técnicos o de datos donde la puntuación puede no ser crucial.\n",
        "\n",
        "2. Limpiar el texto puede ayudar a estandarizarlo y reducir la variabilidad, lo que puede ser útil para mejorar la eficiencia del modelo y su capacidad para generalizar.\n",
        "\n",
        "Lo ideal sería mantener los signos de puntuación para generar un texto más coherente y real, pero esto se traduce diccionarios y modelos muchos más grandes. Necesitando más capacidad de cómputo para predecir la siguiente letra o palabra, capacidad de cómputo de la cual carecemos. Ya que el objetivo es comparar los enfoques del modelado a nivel de caracteres y a nivel de palabras, optamos por eliminar los signos de puntuación y los números."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9Sa2sT27rzO",
        "outputId": "bce21b20-528a-497f-dacf-1fcf27a79eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del vocabulario (a nivel de caracteres): 27\n",
            "Tamaño del vocabulario (a nivel de palabras): 91343\n",
            "\n",
            "Ejemplo preprocesado:\n",
            "wall st bears claw back into the black reuters reuters  shortsellers wall streets dwindlingband of ultracynics are seeing green again\n",
            "\n",
            "Tokenización a nivel de caracteres (primeros 50 tokens):\n",
            "[20, 3, 10, 10, 1, 6, 4, 1, 21, 2, 3, 9, 6, 1, 13, 10, 3, 20, 1, 21, 3, 13, 23, 1, 5, 8, 4, 7, 1, 4, 12, 2, 1, 21, 10, 3, 13, 23, 1, 9, 2, 14, 4, 2, 9, 6, 1, 9, 2, 14]\n",
            "\n",
            "Tokenización a nivel de palabras (primeros 10 tokens):\n",
            "[391, 324, 1525, 14260, 99, 54, 1, 812, 23, 23]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    # Convertir a minúscula\n",
        "    text = text.lower()\n",
        "    # Remover los caracteres especiales y los dígitos\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Preprocesar los textos\n",
        "df_train['processed_text'] = df_train['text'].apply(preprocess_text)\n",
        "df_test['processed_text'] = df_test['text'].apply(preprocess_text)\n",
        "\n",
        "# Tokenización a nivel de caracter\n",
        "char_tokenizer = Tokenizer(char_level=True)\n",
        "char_tokenizer.fit_on_texts(df_train['processed_text'])\n",
        "\n",
        "# Tokenización a nivel de palabras\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(df_train['processed_text'])\n",
        "\n",
        "print(\"Tamaño del vocabulario (a nivel de caracteres):\", len(char_tokenizer.word_index))\n",
        "print(\"Tamaño del vocabulario (a nivel de palabras):\", len(word_tokenizer.word_index))\n",
        "\n",
        "# Mostrar ejemplos preprocesados\n",
        "sample_text = df_train['processed_text'].iloc[0]\n",
        "print(\"\\nEjemplo preprocesado:\")\n",
        "print(sample_text)\n",
        "\n",
        "# Mostrar versiones tokenizadas\n",
        "char_seq = char_tokenizer.texts_to_sequences([sample_text])[0][:50]\n",
        "word_seq = word_tokenizer.texts_to_sequences([sample_text])[0][:10]\n",
        "\n",
        "print(\"\\nTokenización a nivel de caracteres (primeros 50 tokens):\")\n",
        "print(char_seq)\n",
        "print(\"\\nTokenización a nivel de palabras (primeros 10 tokens):\")\n",
        "print(word_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMj0TH-m99yw"
      },
      "source": [
        "# Sección 2: Definición de los modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt7zrsSX9_Z3"
      },
      "source": [
        "## 2.1 Modelo a Nivel de Caracteres\n",
        "Nuestro modelo de Redes Neuronales Recurrentes (RNN) a nivel de caracteres estará basado en celdas LSTM (memoria larga a corto plazo). Aprenden a predecir el siguiente caracter de una secuencia, capturando patrones y relaciones entre caracteres. Las LSTM son particularmente útiles porque pueden mantener dependencias a largo plazo en el texto, lo cual es crucial para generar contenido coherente. En este modelo, cada carácter se representa como un vector codificado one-hot y la red aprende a predecir la distribución de probabilidad del siguiente carácter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dGKwoXk-Dm4",
        "outputId": "71799b98-1223-4735-d7e0-2bc918202f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen del modelo a nivel de caracteres:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 50)           1400      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100, 128)          91648     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 28)                3612      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 228244 (891.58 KB)\n",
            "Trainable params: 228244 (891.58 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Definir parámetros\n",
        "char_vocab_size = len(char_tokenizer.word_index) + 1\n",
        "char_embedding_dim = 50\n",
        "char_lstm_units = 128\n",
        "char_sequence_length = 100\n",
        "\n",
        "# Construir el modelo a nivel de caracteres\n",
        "char_model = Sequential([\n",
        "    Embedding(char_vocab_size, char_embedding_dim, input_length=char_sequence_length),\n",
        "    LSTM(char_lstm_units, return_sequences=True),\n",
        "    LSTM(char_lstm_units),\n",
        "    Dense(char_vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "char_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Mostrar resumen del modelo\n",
        "print(\"Resumen del modelo a nivel de caracteres:\")\n",
        "char_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUQcoPHkfagW"
      },
      "source": [
        "### **Explicación del modelo**\n",
        "#### **Capa de Embeddings**\n",
        "Esta capa convierte el vector one-hot disperso en un embadding, una representación densa y significativa. El modelo ve a cada caracter como un conjunto complejo de características. Estas características capturan relaciones sutiles entre caracters que el modelo puede usar más adelante.\n",
        "\n",
        "#### Parámetros\n",
        "\n",
        "**char_vocab_size:** Representa el tamaño del vocabulario de caracteres. Es el número total de caracteres únicos en el conjunto de datos, más uno para el token de inicio.\n",
        "\n",
        "**char_embedding_dim:** La dimensión de los embeddings de caracteres es 50. Esto significa que cada carácter se representa como un vector de 50 dimensiones.\n",
        "\n",
        "**char_sequence_length:** La longitud de la secuencia es 100. Esto indica que el modelo procesará secuencias de 100 caracteres a la vez.\n",
        "\n",
        "#### **Primera capa LSTM:**\n",
        "Esta capa actúa como un recopilador de contexto. A medida que lee la secuencia de embeddings, realiza un seguimiento de patrones y relaciones importantes en distancias cortas y largas en el texto.\n",
        "\n",
        "#### Parámetros\n",
        "\n",
        "**char_lstm_units:** Cada capa LSTM tendrá 128 unidades, lo que define la capacidad de la red para captar patrones y dependencias en los datos.\n",
        "\n",
        "**return_sequences=True:** Indica que la capa LSTM debe devolver la secuencia completa de salidas para cada paso de tiempo, no solo la última. Es necesario para que la próxima capa LSTM procese toda la secuencia de manera recurrente.\n",
        "\n",
        "#### **Segunda capa LSTM:**\n",
        "Esta capa es un analizador más profundo. Toma la información contextual del primer LSTM y la refina aún más.\n",
        "\n",
        "#### **Capa densa:**\n",
        "Esta capa final densa es el predictor. Considera todos los caracteres posibles y asigna una probabilidad a cada uno, basándose en todo lo que ha aprendido de la secuencia.\n",
        "\n",
        "#### Parámetros\n",
        "\n",
        "**units = char_vocab_size:** El número de unidades en la capa densa es igual al tamaño del vocabulario. Esto asegura que haya una probabilidad de salida para cada carácter en el vocabulario.\n",
        "\n",
        "**activation='softmax':** La activación softmax convierte las salidas en probabilidades, sumando a 1, para la predicción del siguiente carácter en la secuencia.\n",
        "\n",
        "#### **Configuración del modelo**\n",
        "\n",
        "#### Parámetros\n",
        "\n",
        "**loss='categorical_crossentropy':** Se utiliza la función de pérdida de entropía cruzada categórica porque estamos trabajando con un problema de clasificación múltiple (predicción del siguiente carácter).\n",
        "\n",
        "**optimizer='adam':** Se utiliza el optimizador Adam, que es el algoritmo de optimización que ajusta individualmente los pesos del modelo basado en el error y con momentum, proporcionando una convergencia rápida y estable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbMssWm8-FFV"
      },
      "source": [
        "## 2.2 Modelo a nivel de palabras\n",
        "\n",
        "El modelo a nivel de palabras opera en un nivel más alto de abstracción en comparación con el modelo a nivel de caracter. En lugar de predecir el siguiente carácter, predice la siguiente palabra en una secuencia. Este enfoque tiene el potencial de capturar las relaciones semánticas entre palabras de manera más efectiva. Cada palabra se representa como un embedding, lo que permite que el modelo aprenda representaciones significativas de palabras en un espacio vectorial continuo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuwMmEe4-KbY",
        "outputId": "557e7c2f-0559-45b9-c074-b850881977cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Resumen del modelo a nivel de palabras:\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 100)           9134400   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 20, 256)           365568    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 91344)             23475408  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33500688 (127.79 MB)\n",
            "Trainable params: 33500688 (127.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Definir parámetros\n",
        "word_vocab_size = len(word_tokenizer.word_index) + 1\n",
        "word_embedding_dim = 100\n",
        "word_lstm_units = 256\n",
        "word_sequence_length = 20\n",
        "\n",
        "# Construir el modelo a nivel de palabras\n",
        "word_model = Sequential([\n",
        "    Embedding(word_vocab_size, word_embedding_dim, input_length=word_sequence_length),\n",
        "    LSTM(word_lstm_units, return_sequences=True),\n",
        "    LSTM(word_lstm_units),\n",
        "    Dense(word_vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "word_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Mostrar el resumen del modelo:\n",
        "print(\"\\nResumen del modelo a nivel de palabras:\")\n",
        "word_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzrNxrvf1W8_"
      },
      "source": [
        "### **Explicación del modelo**\n",
        "\n",
        "#### **Capa de Embedding:**\n",
        "Toma cada vector de palabra indexada y la transforma en una representación compacta y signigicativa. En lugar de ver las palabras como unidades aisladas, el modelo ahora percibe cada palabra como un conjunto de características. Estas características capturan las relaciones semánticas entre palabras, lo que permite que el modelo comprenda matices como sinónimos, antónimos o palabras que se usan a menudo en contextos similares.\n",
        "\n",
        "#### Parámetros\n",
        "\n",
        "**word_vocab_size:** Es el tamaño del vocabulario de palabras. Es el número total de palabras únicas en el conjunto de datos más uno, para un índice adicional para el token de inicio (o final).\n",
        "\n",
        "**word_embedding_dim:** La dimensión de los embeddings de palabras es 100. Cada palabra se representa como un vector de 100 elementos.\n",
        "\n",
        "**word_sequence_length:** La longitud de la secuencia es 20. Esto significa que el modelo procesa secuencias de 20 palabras a la vez.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "El resto es igual que en el modelo a nivel de caracteres, con las salvedades de lo que representa cada vector de embedding.\n",
        "\n",
        "En esencia, este modelo consiste en leer una secuencia de palabras, desarrollar una comprensión del significado, el contexto y la estructura del texto y luego utilizar esa comprensión para predecir la siguiente palabra.\n",
        "\n",
        "La principal diferencia con el modelo a nivel de caracteres es que éste opera en un nivel superior de abstracción. En lugar de aprender sobre las relaciones entre caracteres, se aprende sobre las relaciones entre palabras. Esto le permite capturar patrones lingüísticos más complejos y potencialmente comprender conceptos de nivel superior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvsX4QqJ-2Z5"
      },
      "source": [
        "# Sección 3: Entrenamiento de los modelos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el-Ql3Ui-6V7"
      },
      "source": [
        "## 3.1 Generación de datos\n",
        "La preparación de datos para entrenar modelos secuenciales implica la creación de pares de entrada y salida donde la entrada es una secuencia de caracteres o palabras y la salida es el siguiente carácter o palabra. Vamos a usar el enfoque de ventana deslizante para generar estas secuencias a partir de los datos de los textos preprocesados. Para el modelo a nivel de caracteres, crearemos secuencias de caracteres superpuestas. Para el modelo a nivel de palabras, crearemos secuencias de palabras. Este proceso permite que nuestros modelos aprendan a predecir el siguiente token en función del contexto anterior.\n",
        "\n",
        "Para manejar grandes conjuntos de datos de manera eficiente, usaremos generadores para crear nuestras secuencias de entrenamiento. Este enfoque nos permite generar datos sobre la marcha durante el entrenamiento, lo que reduce significativamente el uso de memoria y optimizar el uso de recurosos de los cuales carecemos. Crearemos generadores separados para modelos a nivel de caracteres y a nivel de palabras, cada uno de los cuales generará lotes de pares de entrada y salida según sea necesario durante el proceso de capacitación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLvK6XTm-9w1",
        "outputId": "eca52fb7-8d32-4e8e-aba8-9cce55013d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pasos por epoch para el modelo a nivel de caracteres: 118185\n",
            "Pasos por epoch para el modelo a nivel de palabras: 15722\n",
            "\n",
            "Forma del batch de ejemplo del modelo a nivel de caracteres:\n",
            "X: (33, 100), y: (33, 28)\n",
            "\n",
            "Forma del batch de ejemplo del modelo a nivel de palabras:\n",
            "X: (15, 20), y: (15, 91344)\n"
          ]
        }
      ],
      "source": [
        "def sequence_generator(texts, tokenizer, seq_length, batch_size, char_level=True):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    while True:\n",
        "        for sequence in sequences:\n",
        "            n_samples = len(sequence) - seq_length\n",
        "            if n_samples <= 0:\n",
        "                continue\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                batch_end = min(i + batch_size, n_samples)\n",
        "                X = np.zeros((batch_end - i, seq_length), dtype=np.int32)\n",
        "                y = np.zeros((batch_end - i, vocab_size), dtype=np.float32)\n",
        "                for j in range(i, batch_end):\n",
        "                    X[j-i] = sequence[j:j+seq_length]\n",
        "                    y[j-i, sequence[j+seq_length]] = 1\n",
        "                yield X, y\n",
        "\n",
        "# Crear generadores\n",
        "char_seq_gen = sequence_generator(df_train['processed_text'], char_tokenizer, char_sequence_length, 128)\n",
        "word_seq_gen = sequence_generator(df_train['processed_text'], word_tokenizer, word_sequence_length, 128, char_level=False)\n",
        "\n",
        "# Calcular pasos por epoch\n",
        "char_steps = sum(max(0, len(seq) - char_sequence_length) for seq in char_tokenizer.texts_to_sequences(df_train['processed_text'])) // 128\n",
        "word_steps = sum(max(0, len(seq) - word_sequence_length) for seq in word_tokenizer.texts_to_sequences(df_train['processed_text'])) // 128\n",
        "\n",
        "print(f\"Pasos por epoch para el modelo a nivel de caracteres: {char_steps}\")\n",
        "print(f\"Pasos por epoch para el modelo a nivel de palabras: {word_steps}\")\n",
        "\n",
        "# Mostrar un batch de ejemplo\n",
        "char_sample_X, char_sample_y = next(char_seq_gen)\n",
        "word_sample_X, word_sample_y = next(word_seq_gen)\n",
        "\n",
        "print(\"\\nForma del batch de ejemplo del modelo a nivel de caracteres:\")\n",
        "print(f\"X: {char_sample_X.shape}, y: {char_sample_y.shape}\")\n",
        "print(\"\\nForma del batch de ejemplo del modelo a nivel de palabras:\")\n",
        "print(f\"X: {word_sample_X.shape}, y: {word_sample_y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4ZT0QetfZji"
      },
      "source": [
        "#### **Pasos por Época**\n",
        "\n",
        "#### Modelo a Nivel de Caracteres\n",
        "\n",
        "**Pasos por epoch: 118185**\n",
        "\n",
        "El número de pasos por epoch para el modelo a nivel de caracteres es 118185. Esto indica que, durante cada epoch de entrenamiento, el modelo va a recibir 118185 lotes de datos generados secuencialmente.\n",
        "\n",
        "#### Modelo a Nivel de Palabras\n",
        "\n",
        "**Pasos por época: 15722**\n",
        "\n",
        "El número de pasos por época para el modelo a nivel de palabras es 15722. Esto significa que, en cada época de entrenamiento, el modelo procesará 15722 lotes de datos.\n",
        "\n",
        "#### Comparación:\n",
        "\n",
        "El modelo a nivel de caracteres tiene significativamente más pasos por época que el modelo a nivel de palabras. Esto es porque las secuencias de caracteres son más cortas que las secuencias de palabras y el texto se descompone en más secuencias cuando se considera a nivel de caracteres.\n",
        "\n",
        "#### **Forma del Lote de Ejemplo**\n",
        "\n",
        "#### Modelo a Nivel de Caracteres\n",
        "\n",
        "**Forma de X: (33, 100)**\n",
        "\n",
        "X es la matriz de entrada para el lote actual. El tamaño del lote es 33, lo que significa que hay 33 secuencias de caracteres en este lote. Cada secuencia tiene una longitud de 100 caracteres.\n",
        "\n",
        "**Forma de y: (33, 28)**\n",
        "\n",
        "Y es la matriz de salida (etiquetas) para el lote actual. El número de secuencias en el lote es 33, igual al tamaño del lote de X. Cada fila en y es un vector one-hot con 28 posiciones, representando cada posible carácter en el vocabulario. El vocabulario de caracteres incluye 28 caracteres únicos, por lo que cada vector one-hot tiene una dimensión de 28.\n",
        "\n",
        "#### Modelo a Nivel de Palabras\n",
        "\n",
        "**Forma de X: (15, 20)**\n",
        "\n",
        "El tamaño del lote es 15, indicando que hay 15 secuencias de palabras en este lote. Cada secuencia tiene una longitud de 20 palabras.\n",
        "\n",
        "**Forma de y: (15, 91344)**\n",
        "\n",
        "El número de secuencias en el lote es igual a 15, igual al tamaño del lote de X.\n",
        "Cada fila en y es un vector one-hot con 91344 posiciones, representando cada posible palabra en el vocabulario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZSUCfy-_KZ"
      },
      "source": [
        "## 3.2 Entrenamiendo de los modelos\n",
        "El entrenamiento de modelos secuenciales implica alimentar las secuencias de entrada a la red, comparar el siguiente token predicho con el siguiente token real y ajustar los pesos del modelo para minimizar esta diferencia. Usaremos el optimizador Adam y la función de pérdida categorical cross-entropy para ambos modelos. Es importante monitorear el proceso de capacitación para evitar un sobreajuste, por lo que usaremos datos de validación y detención anticipada (Early Stopping). Debido al gran conjunto de datos, utilizaremos el entrenamiento por lotes para que el proceso sea más eficiente en cuanto a memoria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLZgi2UOmS91"
      },
      "source": [
        "### 3.2.1. Configurar los checkpoints y definir el Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya28m0Bhk1Cc"
      },
      "outputs": [],
      "source": [
        "# Crear directorios para checkpoints\n",
        "os.makedirs('checkpoints/char_model', exist_ok=True)\n",
        "os.makedirs('checkpoints/word_model', exist_ok=True)\n",
        "\n",
        "# Configurar el Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Configurar el checkpointing para el modelo a nivel de caracteres\n",
        "char_checkpoint = ModelCheckpoint(\n",
        "    'checkpoints/char_model/model.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Configurar el checkpointing para el modelo a nivel de caracteres\n",
        "word_checkpoint = ModelCheckpoint(\n",
        "    'checkpoints/word_model/model.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GLBS0rm_Il"
      },
      "source": [
        "### 3.2.2. Cargar los pesos del checkpoint si existiesen para el modelo a nivel de caracteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_RYo5IqOcsX",
        "outputId": "7c85652f-b1bd-442f-86cd-b4f1fbdd3e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No se encontraron elementos en el directorio checkpoints.\n"
          ]
        }
      ],
      "source": [
        "# Cargar el mejor modelo a nivel de caracteres\n",
        "try:\n",
        "    best_char_model_path = f'checkpoints/char_model/{sorted(os.listdir(\"checkpoints/char_model\"))[-1]}'\n",
        "    # Verificar si existe un checkpoint, cargar los pesos y mostrar la pérdida inicial\n",
        "    if os.path.exists(best_char_model_path):\n",
        "        print(\"Cargando el checkpoint del modelo a nivel de caracteres...\")\n",
        "\n",
        "        char_model.load_weights(best_char_model_path)\n",
        "        print(\"Checkpoint cargado. Evaluando el modelo...\")\n",
        "\n",
        "        evaluation = word_model.evaluate(word_seq_gen, steps=word_steps // 10)\n",
        "        print(f\"Pérdida inicial luego de cargar el checkpoint: {evaluation}\")\n",
        "\n",
        "except IndexError:\n",
        "    print(\"No se encontraron elementos en el directorio checkpoints.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lQdDL49nIwn"
      },
      "source": [
        "### 3.2.3. Entrenar el modelo a nivel de caracteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2WHGd7z_clU"
      },
      "outputs": [],
      "source": [
        "# Entrenar el modelo a nivel de caracteres\n",
        "print(\"Entrenando del modelo a nivel de caracteres...\")\n",
        "char_history = char_model.fit(\n",
        "    char_seq_gen,\n",
        "    steps_per_epoch=char_steps,\n",
        "    epochs=10,\n",
        "    validation_data=char_seq_gen,\n",
        "    validation_steps=char_steps // 10,\n",
        "    callbacks=[early_stopping, char_checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvorSnmBniys"
      },
      "source": [
        "### 3.2.4. Cargar los pesos del checkpoint si existiesen para el modelo a nivel de palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qXUfyb1G_W8",
        "outputId": "9f2cea61-2432-4df0-a139-1f40753bd963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No se encontraron elementos en el directorio checkpoints.\n"
          ]
        }
      ],
      "source": [
        "# Cargar el mejor modelo a nivel de palabras\n",
        "try:\n",
        "    best_word_model_path = f'checkpoints/word_model/{sorted(os.listdir(\"checkpoints/word_model\"))[-1]}'\n",
        "\n",
        "    # Verificar si existe un checkpoint, cargar los pesos y mostrar la pérdida inicial\n",
        "    if os.path.exists(best_word_model_path):\n",
        "        print(\"Cargando el checkpoint del modelo a nivel de palabras...\")\n",
        "\n",
        "        word_model.load_weights(best_word_model_path)\n",
        "        print(\"Checkpoint cargado. Evaluando el modelo...\")\n",
        "\n",
        "        evaluation = word_model.evaluate(word_seq_gen, steps=word_steps // 10)\n",
        "        print(f\"Pérdida inicial luego de cargar el checkpoint: {evaluation}\")\n",
        "except IndexError:\n",
        "    print(\"No se encontraron elementos en el directorio checkpoints.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eXz4Pu4pgSi"
      },
      "source": [
        "### 3.2.5. Entrenar el modelo a nivel de palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "pSHlteP_KKKk",
        "outputId": "2f060257-6498-43d4-eca0-07d8039d1a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando del modelo a nivel de palabras...\n",
            "Epoch 1/10\n",
            "15722/15722 [==============================] - ETA: 0s - loss: 5.7492\n",
            "Epoch 1: val_loss improved from inf to 6.12193, saving model to checkpoints/word_model/model.01-6.12.h5\n",
            "15722/15722 [==============================] - 320s 20ms/step - loss: 5.7492 - val_loss: 6.1219\n",
            "Epoch 2/10\n",
            "15720/15722 [============================>.] - ETA: 0s - loss: 5.5238\n",
            "Epoch 2: val_loss improved from 6.12193 to 5.93964, saving model to checkpoints/word_model/model.02-5.94.h5\n",
            "15722/15722 [==============================] - 303s 19ms/step - loss: 5.5237 - val_loss: 5.9396\n",
            "Epoch 3/10\n",
            "15722/15722 [==============================] - ETA: 0s - loss: 5.6882\n",
            "Epoch 3: val_loss did not improve from 5.93964\n",
            "15722/15722 [==============================] - 301s 19ms/step - loss: 5.6882 - val_loss: 6.0138\n",
            "Epoch 4/10\n",
            "15722/15722 [==============================] - ETA: 0s - loss: 5.8944\n",
            "Epoch 4: val_loss did not improve from 5.93964\n",
            "15722/15722 [==============================] - 300s 19ms/step - loss: 5.8944 - val_loss: 6.0110\n",
            "Epoch 5/10\n",
            "  383/15722 [..............................] - ETA: 4:39 - loss: 5.9489"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4cb1c41baac4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Entrenar el modelo a nivel de palabras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Entrenando del modelo a nivel de palabras...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m word_history = word_model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mword_seq_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Entrenar el modelo a nivel de palabras\n",
        "print(\"Entrenando del modelo a nivel de palabras...\")\n",
        "word_history = word_model.fit(\n",
        "    word_seq_gen,\n",
        "    steps_per_epoch=word_steps,\n",
        "    epochs=10,\n",
        "    validation_data=word_seq_gen,\n",
        "    validation_steps=word_steps // 10,\n",
        "    callbacks=[early_stopping, word_checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbwDUXsVrJ0t"
      },
      "source": [
        "# Sección 4: Generación de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsZT_A9boVX1"
      },
      "source": [
        "## 4.1 Generación a nivel de caracteres\n",
        "\n",
        "La generación de texto a nivel de caracteres implica predecir un carácter a la vez en función de la secuencia de caracteres anterior. El modelo toma una secuencia semilla como entrada y genera nuevos caracteres uno por uno, incorporando cada vez el carácter recién generado en la secuencia de entrada para la siguiente predicción. Este proceso continúa hasta que alcanzamos la longitud deseada o una condición de parada. La temperatura se utiliza para controlar la aleatoriedad del texto generado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjcd5Lj8pWB1",
        "outputId": "d88e247b-2e23-4be5-b7c2-3c5d6b9e3d11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testos generados con el modelo a nivel de caracteres:\n",
            "\n",
            "Semilla: The future of artificial intelligence\n",
            "The future of artificial intelligence showed against investy is a trade plidged to vrice new family longs as win russies major its top peace finally handsets for seconds on moneyan said after part of the onedolessy there pass to be decid\n",
            "--------------------------------------------------\n",
            "\n",
            "Semilla: In a world where technology\n",
            "In a world where technology reaccosegry of allows to lesss for his tricken attacks for the microy las of an investigation of mosibility stocks fodd explored prosecutorations out in nithpust declared dips revale on the products \n",
            "--------------------------------------------------\n",
            "\n",
            "Semilla: The most important stocks\n",
            "The most important stocks are its hesproving next week ago but at offering quot turning in the springfield in recent forced as solutions china weather machines soldweatrum plans to on the savitys israeli said on sunday in the\n",
            "--------------------------------------------------\n",
            "\n",
            "Semilla: The global economy faces\n",
            "The global economy faceslywy decade organising podour internet innigacting the finged outlets business sected gyett known a vesses scoring up the wived to years as coming the plane to prime might clanchase of january in minu\n",
            "--------------------------------------------------\n",
            "\n",
            "Semilla: Space exploration \n",
            "Space exploration presss that hope of what would not after coloncks decide points quot of microsoft said on friday that said on friday on once and asianapularacy tricol season system about eive agreement that kicking o\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def generate_text_char(model, tokenizer, seed_text, num_chars=200, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(num_chars):\n",
        "        # Tokenizar la secuencia actual\n",
        "        x = tokenizer.texts_to_sequences([generated_text[-char_sequence_length:]])[0]\n",
        "        x = pad_sequences([x], maxlen=char_sequence_length, padding='pre', truncating='pre')\n",
        "\n",
        "        # Predecir el siguiente caracter\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "\n",
        "        # Aplicar muestreo de temperatura\n",
        "        predictions = np.log(predictions) / temperature\n",
        "        exp_preds = np.exp(predictions)\n",
        "        predictions = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Obtener el caracter siguiente\n",
        "        next_index = np.random.choice(len(predictions), p=predictions)\n",
        "        next_char = tokenizer.index_word.get(next_index, ' ')  # Por defecto es un espacio\n",
        "\n",
        "        # Añadir el caracter generado\n",
        "        generated_text += next_char\n",
        "\n",
        "    generated_text += \".\"\n",
        "    return generated_text\n",
        "\n",
        "# Cargar el mejor modelo a nivel de caracteres\n",
        "best_char_model_path = f'checkpoints/char_model/{sorted(os.listdir(\"checkpoints/char_model\"))[-1]}'\n",
        "char_model.load_weights(best_char_model_path)\n",
        "\n",
        "# Gernerar texto usando el modelo a nivel de caracteres\n",
        "seed_texts = [\n",
        "    \"The future of artificial intelligence\",\n",
        "    \"In a world where technology\",\n",
        "    \"The most important stocks\",\n",
        "    \"The global economy faces\",\n",
        "    \"Space exploration \"\n",
        "]\n",
        "\n",
        "print(\"Testos generados con el modelo a nivel de caracteres:\")\n",
        "for seed in seed_texts:\n",
        "    generated_text = generate_text_char(char_model, char_tokenizer, seed)\n",
        "    print(f\"\\nSemilla: {seed}\")\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUIZnVWrplCQ"
      },
      "source": [
        "## 4.2 Generación a nivel de palabras\n",
        "\n",
        "La generación de texto a nivel de palabra es similar a la generación a nivel de caracteres, pero predice palabras completas en lugar de caracteres individuales. Esto se espera que de como resultado un texto más coherente en secuencias más largas, ya que el modelo ha aprendido patrones y relaciones a nivel de palabras. Sin embargo, puede producir resultados menos diversos en comparación con los modelos a nivel de personaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqQTcVzBNLoH",
        "outputId": "68fefbee-9f26-4203-f197-08aaadb1774d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Textos generados a nivel de palabras:\n",
            "\n",
            "Semilla: The future of artificial intelligence\n",
            "The future of artificial intelligence site a person said on wednesday but it that was paying back out of their bush administration s work in an order to ensure a joint collective bargaining agency an injunction but why may on violent it appears to reduce global warming contained such as cell and gulf of the\n",
            "--------------------------------------------------\n",
            "\n",
            "Semilla: In a world where technology\n",
            "In a world where technology the metres spacecraft is currently in a north korea break to murder people one major rules from scientists and scandals to carry out of the senate along with landslides out of a canadian international delays by murder an american hospital reported the tiny neighbours and toppled marines were in the\n",
            "--------------------------------------------------\n",
            "\n",
            "Semilla: The most important stocks\n",
            "The most important stocks of a milky century in a field pharmacies in the latest wave of other neighbours healing after civilians a of and other severe evidence of murder in merger evidence of a deaths of muslim attacks and people including a us marines victor and vicious livermore on the country s fears\n",
            "--------------------------------------------------\n",
            "\n",
            "Semilla: The global economy faces\n",
            "The global economy faces tuesday s lead to the helm of a radio corporation mena reports lamy that included to recent lead for competition and a joint investigation of terror facilities from the main department in an freestyle on wednesday as an almost former areas of the union a political group month in another\n",
            "--------------------------------------------------\n",
            "\n",
            "Semilla: Space exploration \n",
            "Space exploration to swim officials s territory by the university of medical games has found on their piracy to northern california and china a new explosion and rock and consumer institutions may be respected militants in the holy country pacific here yesterday a day ago the first agency abortive us forces from\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def generate_text_word(model, tokenizer, seed_text, num_words=50, temperature=1.0):\n",
        "    generated_text = seed_text.split()\n",
        "    for _ in range(num_words):\n",
        "        # Tokenizar la secuencia actual\n",
        "        x = tokenizer.texts_to_sequences([' '.join(generated_text[-word_sequence_length:])])[0]\n",
        "        x = pad_sequences([x], maxlen=word_sequence_length, padding='pre', truncating='pre')\n",
        "\n",
        "        # Predecir la siguiente palabra\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "\n",
        "        # Aplicar muestreo de temperatura\n",
        "        predictions = np.log(predictions) / temperature\n",
        "        exp_preds = np.exp(predictions)\n",
        "        predictions = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Obtener la siguiente palabra\n",
        "        next_index = np.random.choice(len(predictions), p=predictions)\n",
        "        next_word = tokenizer.index_word.get(next_index, '<UNK>')  # Usar <UNK> para palabras desconocidas\n",
        "\n",
        "        # Añadir la palabra generada\n",
        "        generated_text.append(next_word)\n",
        "\n",
        "    generated_text += \".\"\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Cargar el mejor modelo a nivel de palabras\n",
        "best_word_model_path = f'checkpoints/word_model/{sorted(os.listdir(\"checkpoints/word_model\"))[-1]}'\n",
        "word_model.load_weights(best_word_model_path)\n",
        "\n",
        "# Generar usando el modelo a nivel de palabras\n",
        "seed_texts = [\n",
        "    \"The future of artificial intelligence\",\n",
        "    \"In a world where technology\",\n",
        "    \"The most important stocks\",\n",
        "    \"The global economy faces\",\n",
        "    \"Space exploration \"\n",
        "]\n",
        "\n",
        "print(\"\\nTextos generados a nivel de palabras:\")\n",
        "for seed in seed_texts:\n",
        "    generated_text = generate_text_word(word_model, word_tokenizer, seed)\n",
        "    print(f\"\\nSemilla: {seed}\")\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv34Ik_F1cJY"
      },
      "source": [
        "# Sección 5: Comparación de modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UK8iU7SHY0k"
      },
      "source": [
        "## **Estructura del Modelo y Representación de Datos**\n",
        "\n",
        "### Modelo a Nivel de Caracteres\n",
        "\n",
        "El modelo a nivel de caracteres procesa el texto como una secuencia de caracteres individuales. El modelo predice el siguiente carácter en la secuencia. La arquitectura utilizada incluye una capa de embeddings para mapear caracteres a vectores que capturen las relaciones entre los caracteres en distintas dimensiones, seguida por dos capas LSTM que capturan la dependencia temporal en la secuencia de caracteres. Finalmente, una capa densa con activación softmax predice la probabilidad del siguiente carácter en el vocabulario.\n",
        "\n",
        "Tras analizar el resultado de los textos generados, se destaca la flexibilidad, pudiendo generar cualquier secuencia de caracteres, lo que es útil para generar textos creativos y menos dependiente de los textos de entrenamiento. La desventaja es que tiene una visión limitada del contexto global debido a que cada carácter por sí solo contiene poca información semántica.\n",
        "\n",
        "Este modelo requiere de más pasos de entrenamiento y es menos eficiente en términos de tiempo de cómputo debido a la mayor longitud de las secuencias. Lo que se tradujo en una mayor dificultad de entrenamiento.\n",
        "\n",
        "### Modelo a Nivel de Palabras\n",
        "El modelo a nivel de palabras procesa el texto como una secuencia de palabras. Cada palabra es representada como un índice en un vocabulario de palabras, y el modelo predice la siguiente palabra en la secuencia. Este enfoque utiliza una capa de embeddings para convertir las palabras en vectores densos y representativos, seguida de dos capas LSTM para capturar dependencias a largo plazo en las secuencias de palabras. La capa de salida es una softmax que predice la probabilidad de la siguiente palabra en el vocabulario.\n",
        "\n",
        "Una ventaja, que pudimos observar con respecto al modelo a nivel de caracteres, es que captura mejor las relaciones semánticas y la estructura gramatical del texto, ya que cada palabra aporta más información semántica que un carácter individual. Esto se tradujo en textos más coherentes. Además Maneja secuencias más cortas, lo que puede resultar en una mayor eficiencia computacional y tiempos de entrenamiento más rápidos.\n",
        "\n",
        "Por otro lado, un vocabulario extenso puede llevar a una mayor dimensionalidad de la salida y a la necesidad de más memoria para almacenar los embeddings y manejar la salida one-hot.\n",
        "\n",
        "## **Resultados y Calidad del Texto Generado**\n",
        "\n",
        "### Modelo a Nivel de Caracteres\n",
        "\n",
        "**Resultados**\n",
        "\n",
        "El modelo a nivel de caracteres tiende a producir una variedad amplia de secuencias de texto, algunas de las cuales pueden no ser lingüísticamente coherentes o tener errores ortográficos. Sin embargo, este es capaz de generar nuevas palabras o combinaciones de caracteres que no se encuentran en los datos de entrenamiento, lo que puede ser útil para la generación creativa o artística.\n",
        "\n",
        "Estimamos que sería eficaz en la reproducción de patrones de caracteres locales, si hubiese sido entrenado incluyendo símbolos y dígitos, lo que permitiría generar texto que siguieran las reglas ortográficas y gramaticales en niveles locales.\n",
        "\n",
        "Por otro lado, la fluidez y coherencia global del texto pueden ser limitadas debido a la falta de contexto semántico amplio.\n",
        "\n",
        "### Modelo a Nivel de Palabras\n",
        "\n",
        "**Resultados**\n",
        "\n",
        "El modelo a nivel de palabras genera textos más coherentes y semánticamente consistentes, con frases y oraciones que tienen sentido global. Así mismo no comete ortográficos y casi no comete  gramaticales al contrario del modelo a nivel de caracteres. Ya que, en primer lugar, predice palabras enteras que ya tienen su ortografía definida y codificada y en segundo, por captar mejor la semántica global.\n",
        "\n",
        "La generación de texto parece ser más predecible y menos creativa, ya que está limitada por el vocabulario de palabras predefinido y está más influenciada por la gramática del conjunto de entrenamiento.\n",
        "\n",
        "Este modelo maneja mejor el contexto y la coherencia a largo plazo, produciendo texto que sigue una estructura lógica y gramatical adecuada pero está limitado por las palabras o frases que están presentes en el vocabulario de entrenamiento, lo que restringe la capacidad de generación creativa.\n",
        "\n",
        "## **Consideraciones para la Aplicación**\n",
        "\n",
        "El modelo a nivel de caracteres es ideal para tareas que requieren creatividad en la generación de texto, como la poesía o la generación de nombres.También es útil para lenguajes técnicos y códigos que requieren precisión a nivel de caracteres.\n",
        "\n",
        "Por otro lado, el modelo a nivel de palabras es preferible para la mayoría de las tareas de procesamiento de lenguaje natural, donde la coherencia semántica y la estructura gramatical son críticas. También parece adecuado para la generación de textos técnicos y artículos que requieren precisión y claridad en el lenguaje. Sumado a esto, al poder captar la semántica global de los textos, es mejor para generar textos largos que requieren mantener el contexto y la coherencia a lo largo de múltiples oraciones o párrafos.\n",
        "\n",
        "## **Otras consideraciones**\n",
        "Las conclusiones volcadas en el texto anterior fueron en parte conjeturas obtenidas de la observación de los resultados y en parte inferidos a través de la comprensión de la arquitectura de la red y de los procesos de entrenamiento y predicción. Esto se debe a que nos vimos limitados por la capacidad de cómputo prestadas por Colab, Kaggle, entre otros, borrando los runtimes sin permitirnos descargar los útimos checkpoints y perdiendo epochs enteras por este motivo. Además, el tiempo de GPU en estos servicios tiene un tiempo de refresco alto, y nuestro tiempo estuvo limitado. Esto nos impuso restricciones en el tamaño del modelo y en la inclusión de símbolos en el entrenamiento. Por estos motivos el texto generado es de una calidad pobrísima y, por ende, difícil de analizar en profundidad y obtener insights más valiosos.\n",
        "\n",
        "De haber tenido a disposición un poder de cómputo mayor hubiéramos propuesto las siguientes al modelo aquí presentado:\n",
        "1. Inclusión de símbolos y dígitos a los diccionarios\n",
        "2. Aumento de unidades LSTM por capa\n",
        "3. Añadir más capas densas con dropout para prevenir el overfitting\n",
        "4. Implementar LSTM bidireccionales\n",
        "5. Usar un learning rate scheduler como ReduceLROnPlateau\n",
        "6. Aumentar la longitud de la secuencia\n",
        "7. Usar arquitecturas más sofisticadas como encoder-decoder, encoder-decoder con atención o incluso transformers.\n",
        "\n",
        "## **Conclusión**\n",
        "En conclusión, ambos enfoques tienen sus ventajas y desventajas dependiendo del contexto y la aplicación específica. El modelo a nivel de caracteres es más flexible y creativo, pero puede carecer de coherencia semántica a largo plazo. Por otro lado, el modelo a nivel de palabras es más efectivos para generar textos coherentes y semánticamente ricos, aunque pueden estar limitados por el tamaño y la composición del vocabulario y las muestras de entrenamiento. La elección entre uno y otro dependerá de las necesidades específicas del problema y del tipo de texto que se desea generar.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "009d9cccf1254057a65ad353aef4943d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02b55025498547ac87f6824e57de530e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04231bc8cfdf48a28f5adb56eada4476": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "069cdb3e96b446d59563ac978d288a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a2e8854a2fd4c02a03350164d143c40",
              "IPY_MODEL_2a16d3de520e426f85cd6c32cbcd5928",
              "IPY_MODEL_5f4954a9810f45c1848622e2a84afaab"
            ],
            "layout": "IPY_MODEL_7927571e627240be85488981fb0f51e4"
          }
        },
        "088a4bd7413d4a48b18f81536d7600b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "092ad9e2ce1746b8a23915d447266787": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a090f95d1aea43cf8d76e022a034b71f",
            "max": 120000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e25597fbb2da4c6e8a4dea142aa5441b",
            "value": 120000
          }
        },
        "0f8f14e9d2bd4d26818fb7034ef72c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02b55025498547ac87f6824e57de530e",
            "placeholder": "​",
            "style": "IPY_MODEL_61a31bb7e49b40abacced828c1f42928",
            "value": " 1.23M/1.23M [00:00&lt;00:00, 5.73MB/s]"
          }
        },
        "17f3cc1a23004343be7a7a0576574a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd0288b0ccec49e387face63d9500fe4",
            "max": 1234829,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93af605ec66348aa950ca8339088ef1f",
            "value": 1234829
          }
        },
        "2a16d3de520e426f85cd6c32cbcd5928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff87dcb39f9749479ee6707021a15c41",
            "max": 18585438,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd4a661a328c4d88b330cd429c473bdf",
            "value": 18585438
          }
        },
        "2bceed6e959f40c880ded258ecfba5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e4e8e2ca8e947c28ee7e669a64afa7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e839e89cf644fad80c31bf57b0aae92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca60b4543fb1411382d838e4a59f27e2",
            "max": 8070,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_472892fc0c634dfc8a419f0a4144b650",
            "value": 8070
          }
        },
        "31493b0f488844b487a4fcd0ae7a75b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68053b71b5241059a3e8c7bcb3a12fb",
            "placeholder": "​",
            "style": "IPY_MODEL_2bceed6e959f40c880ded258ecfba5f2",
            "value": "Downloading data: 100%"
          }
        },
        "37c0eadd792d4a1cab68f709f4a73fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37f81c66c40745bb9ab631e26497ce7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61b3f5b74437464493179b0a0ad4081c",
              "IPY_MODEL_2e839e89cf644fad80c31bf57b0aae92",
              "IPY_MODEL_678087494c8a4951893070cc3fe6c7b6"
            ],
            "layout": "IPY_MODEL_04231bc8cfdf48a28f5adb56eada4476"
          }
        },
        "408c303a9669413baeac7d68bdf3c34d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "424f87b892a849679d46aaa72e8c4f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "472892fc0c634dfc8a419f0a4144b650": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ed3c4f20ff244e39b3aa22a830e1c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce85de7855cd41eabfe242c453e32e48",
            "placeholder": "​",
            "style": "IPY_MODEL_92ced3143a784edab482804388ef4111",
            "value": "Generating test split: 100%"
          }
        },
        "51dd20394cd642cf81f6d075bc9f2717": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f4954a9810f45c1848622e2a84afaab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e4e8e2ca8e947c28ee7e669a64afa7c",
            "placeholder": "​",
            "style": "IPY_MODEL_f643897de20b48fbad55fa3071f8a5ac",
            "value": " 18.6M/18.6M [00:00&lt;00:00, 28.2MB/s]"
          }
        },
        "61a31bb7e49b40abacced828c1f42928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61b3f5b74437464493179b0a0ad4081c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_408c303a9669413baeac7d68bdf3c34d",
            "placeholder": "​",
            "style": "IPY_MODEL_727134da2d334d60bede4a21c73532b2",
            "value": "Downloading readme: 100%"
          }
        },
        "678087494c8a4951893070cc3fe6c7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dfb7b6b00ed4655ae520da8da9348a5",
            "placeholder": "​",
            "style": "IPY_MODEL_7e835ce69df1441698a68d1b0ac5752a",
            "value": " 8.07k/8.07k [00:00&lt;00:00, 387kB/s]"
          }
        },
        "6e0e2e9b8cd7401fbcc87ec81e3dd96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ed3c4f20ff244e39b3aa22a830e1c9e",
              "IPY_MODEL_8984c1830a344060bc745986b620049a",
              "IPY_MODEL_ab9efe27ccde44608feff505d30bd63e"
            ],
            "layout": "IPY_MODEL_f644eafbbba545a6a06fb4bab7e8c5b7"
          }
        },
        "706cb46c74e7410fa98079212f6b4b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71643f493a8348918bc8322707009b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71d0ef96ab1f46f3845e760915022741": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "727134da2d334d60bede4a21c73532b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7927571e627240be85488981fb0f51e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a5d7a153d4841c1ae0590cd8f5b5cf2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dfb7b6b00ed4655ae520da8da9348a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e835ce69df1441698a68d1b0ac5752a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86f22dbb5583476c99a129b05d1488c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71643f493a8348918bc8322707009b2e",
            "placeholder": "​",
            "style": "IPY_MODEL_424f87b892a849679d46aaa72e8c4f41",
            "value": " 120000/120000 [00:00&lt;00:00, 431795.76 examples/s]"
          }
        },
        "8984c1830a344060bc745986b620049a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_088a4bd7413d4a48b18f81536d7600b2",
            "max": 7600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51dd20394cd642cf81f6d075bc9f2717",
            "value": 7600
          }
        },
        "8a2e8854a2fd4c02a03350164d143c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b06117eea5f5488a9697f71b28404a62",
            "placeholder": "​",
            "style": "IPY_MODEL_706cb46c74e7410fa98079212f6b4b01",
            "value": "Downloading data: 100%"
          }
        },
        "8dfddee8ceb74dd09827b3a8a5141a81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92ced3143a784edab482804388ef4111": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93af605ec66348aa950ca8339088ef1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "961f29bcc8364c2887eca79ebb196882": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31493b0f488844b487a4fcd0ae7a75b0",
              "IPY_MODEL_17f3cc1a23004343be7a7a0576574a4c",
              "IPY_MODEL_0f8f14e9d2bd4d26818fb7034ef72c22"
            ],
            "layout": "IPY_MODEL_8dfddee8ceb74dd09827b3a8a5141a81"
          }
        },
        "a090f95d1aea43cf8d76e022a034b71f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab2976587eed41cd9fe0cf22d649029e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e192ceaa760149b080b57209f2793860",
            "placeholder": "​",
            "style": "IPY_MODEL_71d0ef96ab1f46f3845e760915022741",
            "value": "Generating train split: 100%"
          }
        },
        "ab9efe27ccde44608feff505d30bd63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37c0eadd792d4a1cab68f709f4a73fe9",
            "placeholder": "​",
            "style": "IPY_MODEL_009d9cccf1254057a65ad353aef4943d",
            "value": " 7600/7600 [00:00&lt;00:00, 132479.05 examples/s]"
          }
        },
        "b06117eea5f5488a9697f71b28404a62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd0288b0ccec49e387face63d9500fe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca60b4543fb1411382d838e4a59f27e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf34038d780440ba5d41862c16e1c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab2976587eed41cd9fe0cf22d649029e",
              "IPY_MODEL_092ad9e2ce1746b8a23915d447266787",
              "IPY_MODEL_86f22dbb5583476c99a129b05d1488c4"
            ],
            "layout": "IPY_MODEL_7a5d7a153d4841c1ae0590cd8f5b5cf2"
          }
        },
        "ce85de7855cd41eabfe242c453e32e48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4a661a328c4d88b330cd429c473bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e192ceaa760149b080b57209f2793860": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e25597fbb2da4c6e8a4dea142aa5441b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e68053b71b5241059a3e8c7bcb3a12fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f643897de20b48fbad55fa3071f8a5ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f644eafbbba545a6a06fb4bab7e8c5b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff87dcb39f9749479ee6707021a15c41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
